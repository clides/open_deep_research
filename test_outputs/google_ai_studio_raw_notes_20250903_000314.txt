Google AI Studio Integration Test - Raw Notes
Timestamp: 2025-09-03T00:03:14.656679
Query: What model is used in test_openrouter_integration file?
Model: gemini-2.0-flash-lite
================================================================================

RAW NOTE 1:
----------------------------------------

.mypy_cache/3.13/test_openrouter_integration.data.json
{".class":"MypyFile","_fullname":"test_openrouter_integration","future_import_flags":[],"is_partial_stub_package":false,"is_stub":false,"names":{".class":"SymbolTable","MemorySaver":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.MemorySaver","name":"MemorySaver","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.MemorySaver","source_any":null,"type_of_any":3}}},"__annotations__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__annotations__","name":"__annotations__","setter_type":null,"type":{".class":"Instance","args":["builtins.str",{".class":"AnyType","missing_import_name":null,"source_any":null,"type_of_any":6}],"extra_attrs":null,"type_ref":"builtins.dict"}}},"__doc__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__doc__","name":"__doc__","setter_type":null,"type":"builtins.str"}},"__file__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__file__","name":"__file__","setter_type":null,"type":"builtins.str"}},"__name__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__name__","name":"__name__","setter_type":null,"type":"builtins.str"}},"__package__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__package__","name":"__package__","setter_type":null,"type":"builtins.str"}},"__spec__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__spec__","name":"__spec__","setter_type":null,"type":"_frozen_importlib.ModuleSpec"}},"asyncio":{".class":"SymbolTableNode","cross_ref":"asyncio","kind":"Gdef"},"deep_researcher_builder":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.deep_researcher_builder","name":"deep_researcher_builder","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.deep_researcher_builder","source_any":null,"type_of_any":3}}},"load_dotenv":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.load_dotenv","name":"load_dotenv","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.load_dotenv","source_any":null,"type_of_any":3}}},"main":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.main","name":"main","type":null}},"os":{".class":"SymbolTableNode","cross_ref":"os","kind":"Gdef"},"test_api_key_retrieval":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_api_key_retrieval","name":"test_api_key_retrieval","type":null}},"test_model_initialization":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_model_initialization","name":"test_model_initialization","type":null}},"test_openrouter_integration":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_openrouter_integration","name":"test_openrouter_integration","type":null}},"uuid":{".class":"SymbolTableNode","cross_ref":"uuid","kind":"Gdef"}},"path":"/Users/euclides/Documents/Programming/Projects/open_deep_research/src/test_openrouter_integration.py"}

.mypy_cache/3.13/test_openrouter_integration.meta.json
{"data_mtime":1756511441,"dep_lines":[3,4,5,1,1,1,1,1,1,1,8,6,7,83],"dep_prios":[10,10,10,5,30,30,30,30,30,30,5,5,5,20],"dependencies":["asyncio","uuid","os","builtins","_collections_abc","_frozen_importlib","_typeshed","abc","asyncio.events","typing"],"hash":"2306685f8c92a1b79021ff6256b5247c2611879b","id":"test_openrouter_integration","ignore_all":false,"interface_hash":"6b411363c6cfd3982a3ef831cfedc81ee96ef6b2","mtime":1756511441,"options":{"allow_redefinition":true,"allow_redefinition_new":false,"allow_untyped_globals":true,"always_false":[],"always_true":[],"bazel":false,"check_untyped_defs":false,"disable_bytearray_promotion":false,"disable_error_code":["import-untyped","import-not-found","misc","unused-ignore","name-defined","attr-defined","no-untyped-def","no-untyped-call","valid-type"],"disable_memoryview_promotion":false,"disabled_error_codes":["attr-defined","import-not-found","import-untyped","misc","name-defined","no-untyped-call","no-untyped-def","unused-ignore","valid-type"],"disallow_any_decorated":false,"disallow_any_explicit":false,"disallow_any_expr":false,"disallow_any_generics":false,"disallow_any_unimported":false,"disallow_incomplete_defs":false,"disallow_subclassing_any":false,"disallow_untyped_calls":false,"disallow_untyped_decorators":false,"disallow_untyped_defs":false,"enable_error_code":[],"enabled_error_codes":[],"extra_checks":false,"follow_imports":"silent","follow_imports_for_stubs":false,"follow_untyped_imports":false,"ignore_errors":false,"ignore_missing_imports":true,"implicit_optional":false,"implicit_reexport":true,"local_partial_types":false,"mypyc":false,"old_type_inference":false,"platform":"darwin","plugins":[],"strict_bytes":false,"strict_concatenate":false,"strict_equality":false,"strict_optional":false,"warn_no_return":false,"warn_return_any":false,"warn_unreachable":false,"warn_unused_ignores":false},"path":"/Users/euclides/Documents/Programming/Projects/open_deep_research/src/test_openrouter_integration.py","plugin_data":null,"size":7119,"suppressed":["langgraph.checkpoint.memory","dotenv","open_deep_research.deep_researcher","open_deep_research.utils"],"version_id":"1.16.1"}

test_outputs/google_ai_studio_full_report_20250830_104622.txt
Google AI Studio Integration Test - Full Report
Timestamp: 2025-08-30T10:46:22.741827
Query: What query is used in test_openrouter_integration file?
Model: gemini-2.0-flash-lite
================================================================================

The query used in the `test_openrouter_integration` file is:

```
What are the key benefits of renewable energy?
```

### References
[1] /Users/euclides/Documents/Programming/Projects/open_deep_research/src/test_openrouter_integration.py

test_outputs/google_ai_studio_raw_notes_20250830_104622.txt
Google AI Studio Integration Test - Raw Notes
Timestamp: 2025-08-30T10:46:22.741067
Query: What query is used in test_openrouter_integration file?
Model: gemini-2.0-flash-lite
================================================================================

RAW NOTE 1:
----------------------------------------

.mypy_cache/3.13/test_openrouter_integration.data.json
{".class":"MypyFile","_fullname":"test_openrouter_integration","future_import_flags":[],"is_partial_stub_package":false,"is_stub":false,"names":{".class":"SymbolTable","MemorySaver":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.MemorySaver","name":"MemorySaver","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.MemorySaver","source_any":null,"type_of_any":3}}},"__annotations__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__annotations__","name":"__annotations__","setter_type":null,"type":{".class":"Instance","args":["builtins.str",{".class":"AnyType","missing_import_name":null,"source_any":null,"type_of_any":6}],"extra_attrs":null,"type_ref":"builtins.dict"}}},"__doc__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__doc__","name":"__doc__","setter_type":null,"type":"builtins.str"}},"__file__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__file__","name":"__file__","setter_type":null,"type":"builtins.str"}},"__name__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__name__","name":"__name__","setter_type":null,"type":"builtins.str"}},"__package__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__package__","name":"__package__","setter_type":null,"type":"builtins.str"}},"__spec__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__spec__","name":"__spec__","setter_type":null,"type":"_frozen_importlib.ModuleSpec"}},"asyncio":{".class":"SymbolTableNode","cross_ref":"asyncio","kind":"Gdef"},"deep_researcher_builder":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.deep_researcher_builder","name":"deep_researcher_builder","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.deep_researcher_builder","source_any":null,"type_of_any":3}}},"load_dotenv":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.load_dotenv","name":"load_dotenv","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.load_dotenv","source_any":null,"type_of_any":3}}},"main":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.main","name":"main","type":null}},"os":{".class":"SymbolTableNode","cross_ref":"os","kind":"Gdef"},"test_api_key_retrieval":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_api_key_retrieval","name":"test_api_key_retrieval","type":null}},"test_model_initialization":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_model_initialization","name":"test_model_initialization","type":null}},"test_openrouter_integration":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_openrouter_integration","name":"test_openrouter_integration","type":null}},"uuid":{".class":"SymbolTableNode","cross_ref":"uuid","kind":"Gdef"}},"path":"/Users/euclides/Documents/Programming/Projects/open_deep_research/src/test_openrouter_integration.py"}

.mypy_cache/3.13/test_openrouter_integration.meta.json
{"data_mtime":1756511441,"dep_lines":[3,4,5,1,1,1,1,1,1,1,8,6,7,83],"dep_prios":[10,10,10,5,30,30,30,30,30,30,5,5,5,20],"dependencies":["asyncio","uuid","os","builtins","_collections_abc","_frozen_importlib","_typeshed","abc","asyncio.events","typing"],"hash":"2306685f8c92a1b79021ff6256b5247c2611879b","id":"test_openrouter_integration","ignore_all":false,"interface_hash":"6b411363c6cfd3982a3ef831cfedc81ee96ef6b2","mtime":1756511441,"options":{"allow_redefinition":true,"allow_redefinition_new":false,"allow_untyped_globals":true,"always_false":[],"always_true":[],"bazel":false,"check_untyped_defs":false,"disable_bytearray_promotion":false,"disable_error_code":["import-untyped","import-not-found","misc","unused-ignore","name-defined","attr-defined","no-untyped-def","no-untyped-call","valid-type"],"disable_memoryview_promotion":false,"disabled_error_codes":["attr-defined","import-not-found","import-untyped","misc","name-defined","no-untyped-call","no-untyped-def","unused-ignore","valid-type"],"disallow_any_decorated":false,"disallow_any_explicit":false,"disallow_any_expr":false,"disallow_any_generics":false,"disallow_any_unimported":false,"disallow_incomplete_defs":false,"disallow_subclassing_any":false,"disallow_untyped_calls":false,"disallow_untyped_decorators":false,"disallow_untyped_defs":false,"enable_error_code":[],"enabled_error_codes":[],"extra_checks":false,"follow_imports":"silent","follow_imports_for_stubs":false,"follow_untyped_imports":false,"ignore_errors":false,"ignore_missing_imports":true,"implicit_optional":false,"implicit_reexport":true,"local_partial_types":false,"mypyc":false,"old_type_inference":false,"platform":"darwin","plugins":[],"strict_bytes":false,"strict_concatenate":false,"strict_equality":false,"strict_optional":false,"warn_no_return":false,"warn_return_any":false,"warn_unreachable":false,"warn_unused_ignores":false},"path":"/Users/euclides/Documents/Programming/Projects/open_deep_research/src/test_openrouter_integration.py","plugin_data":null,"size":7119,"suppressed":["langgraph.checkpoint.memory","dotenv","open_deep_research.deep_researcher","open_deep_research.utils"],"version_id":"1.16.1"}

test_outputs/google_ai_studio_full_report_20250830_104542.txt
Google AI Studio Integration Test - Full Report
Timestamp: 2025-08-30T10:45:42.085286
Query: What query is used in test_openrouter_integration file?
Model: gemini-2.0-flash-lite
================================================================================

The `test_openrouter_integration.py` file uses the following query:

*   `What are the key benefits of renewable energy?`

### References
[1] /Users/euclides/Documents/Programming/Projects/open_deep_research/src/test_openrouter_integration.py

test_outputs/google_ai_studio_raw_notes_20250830_104542.txt
Google AI Studio Integration Test - Raw Notes
Timestamp: 2025-08-30T10:45:42.084421
Query: What query is used in test_openrouter_integration file?
Model: gemini-2.0-flash-lite
================================================================================

RAW NOTE 1:
----------------------------------------
Okay, I will start by using the `extract_relevant_file_content` tool to get the content of the 'test_openrouter_integration' file.
.mypy_cache/3.13/test_openrouter_integration.data.json
{".class":"MypyFile","_fullname":"test_openrouter_integration","future_import_flags":[],"is_partial_stub_package":false,"is_stub":false,"names":{".class":"SymbolTable","MemorySaver":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.MemorySaver","name":"MemorySaver","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.MemorySaver","source_any":null,"type_of_any":3}}},"__annotations__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__annotations__","name":"__annotations__","setter_type":null,"type":{".class":"Instance","args":["builtins.str",{".class":"AnyType","missing_import_name":null,"source_any":null,"type_of_any":6}],"extra_attrs":null,"type_ref":"builtins.dict"}}},"__doc__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__doc__","name":"__doc__","setter_type":null,"type":"builtins.str"}},"__file__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__file__","name":"__file__","setter_type":null,"type":"builtins.str"}},"__name__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__name__","name":"__name__","setter_type":null,"type":"builtins.str"}},"__package__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__package__","name":"__package__","setter_type":null,"type":"builtins.str"}},"__spec__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__spec__","name":"__spec__","setter_type":null,"type":"_frozen_importlib.ModuleSpec"}},"asyncio":{".class":"SymbolTableNode","cross_ref":"asyncio","kind":"Gdef"},"deep_researcher_builder":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.deep_researcher_builder","name":"deep_researcher_builder","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.deep_researcher_builder","source_any":null,"type_of_any":3}}},"load_dotenv":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.load_dotenv","name":"load_dotenv","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.load_dotenv","source_any":null,"type_of_any":3}}},"main":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.main","name":"main","type":null}},"os":{".class":"SymbolTableNode","cross_ref":"os","kind":"Gdef"},"test_api_key_retrieval":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_api_key_retrieval","name":"test_api_key_retrieval","type":null}},"test_model_initialization":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_model_initialization","name":"test_model_initialization","type":null}},"test_openrouter_integration":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_openrouter_integration","name":"test_openrouter_integration","type":null}},"uuid":{".class":"SymbolTableNode","cross_ref":"uuid","kind":"Gdef"}},"path":"/Users/euclides/Documents/Programming/Projects/open_deep_research/src/test_openrouter_integration.py"}

.mypy_cache/3.13/test_openrouter_integration.meta.json
{"data_mtime":1756511441,"dep_lines":[3,4,5,1,1,1,1,1,1,1,8,6,7,83],"dep_prios":[10,10,10,5,30,30,30,30,30,30,5,5,5,20],"dependencies":["asyncio","uuid","os","builtins","_collections_abc","_frozen_importlib","_typeshed","abc","asyncio.events","typing"],"hash":"2306685f8c92a1b79021ff6256b5247c2611879b","id":"test_openrouter_integration","ignore_all":false,"interface_hash":"6b411363c6cfd3982a3ef831cfedc81ee96ef6b2","mtime":1756511441,"options":{"allow_redefinition":true,"allow_redefinition_new":false,"allow_untyped_globals":true,"always_false":[],"always_true":[],"bazel":false,"check_untyped_defs":false,"disable_bytearray_promotion":false,"disable_error_code":["import-untyped","import-not-found","misc","unused-ignore","name-defined","attr-defined","no-untyped-def","no-untyped-call","valid-type"],"disable_memoryview_promotion":false,"disabled_error_codes":["attr-defined","import-not-found","import-untyped","misc","name-defined","no-untyped-call","no-untyped-def","unused-ignore","valid-type"],"disallow_any_decorated":false,"disallow_any_explicit":false,"disallow_any_expr":false,"disallow_any_generics":false,"disallow_any_unimported":false,"disallow_incomplete_defs":false,"disallow_subclassing_any":false,"disallow_untyped_calls":false,"disallow_untyped_decorators":false,"disallow_untyped_defs":false,"enable_error_code":[],"enabled_error_codes":[],"extra_checks":false,"follow_imports":"silent","follow_imports_for_stubs":false,"follow_untyped_imports":false,"ignore_errors":false,"ignore_missing_imports":true,"implicit_optional":false,"implicit_reexport":true,"local_partial_types":false,"mypyc":false,"old_type_inference":false,"platform":"darwin","plugins":[],"strict_bytes":false,"strict_concatenate":false,"strict_equality":false,"strict_optional":false,"warn_no_return":false,"warn_return_any":false,"warn_unreachable":false,"warn_unused_ignores":false},"path":"/Users/euclides/Documents/Programming/Projects/open_deep_research/src/test_openrouter_integration.py","plugin_data":null,"size":7119,"suppressed":["langgraph.checkpoint.memory","dotenv","open_deep_research.deep_researcher","open_deep_research.utils"],"version_id":"1.16.1"}

test_outputs/google_ai_studio_full_report_20250829_103557.txt
Google AI Studio Integration Test - Full Report
Timestamp: 2025-08-29T10:35:57.681251
Query: What query is used in test_openrouter_integration file?
Model: gemini-2.0-flash-lite
================================================================================

I am unable to provide the specific query used in the `test_openrouter_integration` file because the `<Findings>` section is empty.
### References
[1] /path/to/test_openrouter_integration.py (Hypothetical, as no file path was provided)

.git/index
DIRC      /hh  :         mwr>A1x`2F .env.example      hyhy  =         T騁y1L ui (.github/workflows/claude-code-review.yml  hh  >         jslջ`{UG .github/workflows/claude.yml      hNhN  ?         h*#D/PÆ݁9o 
.gitignore        hRhR  @         ~п*\;֪K 	CLAUDE.md hh  A         )|C*uWm LICENSE   hShS  B         ([8UzX5ɨHtƍ` 	README.md h(h(  D         -1-gZppK$%<_?'O] examples/arxiv.md hh  E         .xcR;MH7 "examples/inference-market-gpt45.md        hh  F         'yoMg:?{G examples/inference-market.md      h^h^  G         5NKKRϑr?p examples/pubmed.md        hwhw  H         '8NOL9 langgraph.json    hbWhbW  I         $Ѵ826oe> pyproject.toml    hh  _         thI
0*p"r &src/code_consultant/code_consultant.py    h4Q"*h4Q"*  F         eEl<6
VH|; $src/code_consultant/configuration.py      h-Kh-K  눜         5<9Kѵ3US0wV|h% src/code_consultant/prompts.py    h3r~oh3r~o  a         oP-%]~Mk src/code_consultant/state.py      hbMhbM  E         c$'έʯ2

? src/code_consultant/utils.py      h,%h,%  L         Ŋ/O
l_P src/legacy/CLAUDE.md      h?h?  M          HU^g6" src/legacy/__init__.py    hLhL  N         땲_'\3Z src/legacy/configuration.py       h h   P         6h,"L src/legacy/files/vibe_code.md     hĉhĉ  Q         MIOEC6l src/legacy/graph.ipynb    hƼhƼ  R         R.>X]PQ9lCA src/legacy/graph.py       hh  S         P]\p3(NR"< src/legacy/legacy.md      hѯhѯ  T        v܊T+ĺ src/legacy/multi_agent.ipynb      hfhf  U         KtV7Be1X( src/legacy/multi_agent.py h|,h|,  V         P׀e89YdMՋ src/legacy/prompts.py     hh  W         j**ꮇmnqdLtb	 src/legacy/state.py       h6h6  Y         zLTLHl src/legacy/tests/conftest.py      hݑhݑ  Z         H4K$h
O>}݁
1 src/legacy/tests/run_test.py      hhhh  [         2t9vjnlϠtR
 'src/legacy/tests/test_report_quality.py   h4h4  \        5h8أ
\	< src/legacy/utils.py       hNh
v           uxJk.9ҔaLq src/mcp_server.py hh  d         UKb֘aIsߧ src/security/auth.py      h,#ìh+j  2         !T6fXbP (src/test_google_ai_studio_integration.py  h Zh G           yOȯgwiߒ\U "src/test_openrouter_integration.py        hh  f         %C59(-pO tests/evaluators.py       h+h+  h        5'Hhx*d ;tests/expt_results/deep_research_bench_claude4-sonnet.jsonl       hW
hW
  i        ^MeyQҳn7Ny$_ 4tests/expt_results/deep_research_bench_gpt-4.1.jsonl      h_h_  j         }(kV4~ei 2tests/expt_results/deep_research_bench_gpt-5.jsonl        h h   k         RgT
ZeT  tests/extract_langsmith_data.py   hc.hc.  l         [tPW#z"cfꩵ  tests/pairwise_evaluation.py      h3h3  m         $|WndZh8 tests/prompts.py  hh  n         ИT}5;8`kyMN tests/run_evaluate.py     hh  o         x|yX 'tests/supervisor_parallel_evaluation.py   hh  p        "qq$=DR1XV uv.lock   TREE   47 4
7(9W)7&src 24 3
򟊣|d3cU^H
legacy 15 2
PO;ef[Kfiles 1 0
K{yuNjlw(Etests 3 0
[D'Jdδd(security 1 0
@_cޢ@R+t"dcode_consultant 5 0
!3cmE	tests 9 1
\3;
!$H|7expt_results 3 0
Y  I1c.github 2 1
N+wͮ_}*[workflows 2 0
Ê>Mp}:kexamples 4 0
Qýs'o}P
F[z#7rw/;^Y#l#H&

src/test_google_ai_studio_integration.py
"""Test file for Google AI Studio integration with Gemini 2.0 Flash-Lite model."""

import asyncio
import json
import os
import uuid
from datetime import datetime

from dotenv import load_dotenv
from langgraph.checkpoint.memory import MemorySaver

from code_consultant.code_consultant import code_consultant
from code_consultant.utils import get_all_tools, get_api_key_for_model

load_dotenv()


async def test_google_ai_studio_integration():
    """Test Google AI Studio integration with Gemini 2.0 Flash-Lite model."""

    # Configure Google AI Studio with Gemini 2.0 Flash-Lite model
    config = {
        "configurable": {
            "thread_id": str(uuid.uuid4()),
            # Google AI Studio configuration
            "analyst_model": "google:gemini-2.0-flash-lite",
            "compression_model": "google:gemini-2.0-flash-lite",
            "final_report_model": "google:gemini-2.0-flash-lite",
            "summarization_model": "google:gemini-2.0-flash-lite",
            # Research parameters - reduced for rate limiting
            "max_structured_output_retries": 2,  # Reduced from 3
            "allow_clarification": False,
            "max_concurrent_tasks": 1,  # Reduced from 2 for rate limiting
            "max_iterations": 1,  # Reduced from 2 for rate limiting
            "max_react_tool_calls": 3,  # Reduced from 5 for rate limiting
            # Token limits - conservative for rate limiting
            "analyst_model_max_tokens": 1000000,  # Reduced from 4000
            "compression_model_max_tokens": 1000000,  # Reduced from 4000
            "final_report_model_max_tokens": 1000000,  # Reduced from 4000
            "summarization_model_max_tokens": 1000000,  # Reduced from 2000
            # mcp server config
            "mcp_config": {
                "url": "http://localhost:8080",  # HTTP URL instead of stdio
                "tools": ["extract_relevant_file_content"],
                "auth_required": False,
            },
            "mcp_prompt": """

**CRITICAL FILE READING INSTRUCTIONS:**
**FOR SUPERVISORS:** When asked about local files (like README, config files, etc.), you MUST delegate these tasks to analysts via ExecuteTask. The analysts have access to the `extract_relevant_file_content` tool. Do NOT refuse local file requests - delegate them!

**FOR ANALYSTS:** When conducting file-related analysis, you have access to:
1. `extract_relevant_file_content(query: str)`: To extract content from relevant files based on a query. The query should be a descriptive sentence about what you are looking for.
2. NEVER provide generic information about files when you can read the actual content.
3. Local file questions should ALWAYS use your file reading tools."""
        }
    }

    tools = await get_all_tools(config)  
    print(f"🔧 Available tools: {[tool.name if hasattr(tool, 'name') else str(tool) for tool in tools]}")  
    print(f"🔧 Total tools loaded: {len(tools)}")

    # Simple test query
    test_query = "What query is used in test_openrouter_integration file?"

    try:
        # Initialize the graph
        graph = code_consultant.compile(checkpointer=MemorySaver())

        print("🧪 Testing Google AI Studio integration with Gemini 2.0 Flash-Lite...")
        print(f"📝 Query: {test_query}")
        print("⚙️  Model: gemini-2.0-flash-lite")
        print("-" * 50)

        # Run the research
        result = await graph.ainvoke(
            {"messages": [{"role": "user", "content": test_query}]}, config
        )
        print(f"Supervisor messages: {result.get('supervisor_messages', [])}")  
        print(f"Task brief: {result.get('task_brief', '')}")

        # Save raw notes to file
        if "raw_notes" in result and result["raw_notes"]:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"google_ai_studio_raw_notes_{timestamp}.txt"

            print(f"📝 Raw notes found: {len(result['raw_notes'])} entries")

            # Create output directory if it doesn't exist
            os.makedirs("test_outputs", exist_ok=True)

            with open(f"test_outputs/{filename}", "w", encoding="utf-8") as f:
                f.write(f"Google AI Studio Integration Test - Raw Notes\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Query: {test_query}\n")
                f.write(f"Model: gemini-2.0-flash-lite\n")
                f.write("=" * 80 + "\n\n")

                for i, note in enumerate(result["raw_notes"]):
                    f.write(f"RAW NOTE {i+1}:\n")
                    f.write("-" * 40 + "\n")
                    f.write(str(note))
                    f.write("\n\n" + "=" * 80 + "\n\n")

            print(f"💾 Raw notes saved to: test_outputs/{filename}")

            # Also print a preview
            for i, note in enumerate(result["raw_notes"]):
                print(f"📄 Raw Note {i+1} Preview: {str(note)[:200]}...")
        else:
            print("📝 No raw notes found in result")
            print(f"Available result keys: {list(result.keys())}")

        # Check results
        if "final_report" in result and result["final_report"]:
            print("✅ SUCCESS: Google AI Studio integration working!")
            print(f"📄 Report length: {len(result['final_report'])} characters")

            # Also save the full report
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"google_ai_studio_full_report_{timestamp}.txt"

            with open(f"test_outputs/{report_filename}", "w", encoding="utf-8") as f:
                f.write(f"Google AI Studio Integration Test - Full Report\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Query: {test_query}\n")
                f.write(f"Model: gemini-2.0-flash-lite\n")
                f.write("=" * 80 + "\n\n")
                f.write(result["final_report"])

            print(f"💾 Full report saved to: test_outputs/{report_filename}")

            print("\n📋 Generated Report Preview:")
            print("-" * 30)
            # Show first 500 characters of the report
            preview = result["final_report"][:500]
            print(f"{preview}...")
            print("-" * 30)

            return True
        else:
            print("❌ FAILED: No final report generated")
            print(f"Result keys: {list(result.keys())}")
            return False

    except Exception as e:
        print(f"❌ ERROR: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        return False


async def test_api_key_retrieval():
    """Test that the get_api_key_for_model function works for Google models."""

    try:
        print("\n🔍 Testing API key retrieval...")

        # Mock config for testing
        mock_config = {"configurable": {}}

        # Test Google key retrieval
        api_key = get_api_key_for_model("google:gemini-2.0-flash-lite", mock_config)

        if api_key:
            print(f"✅ API key retrieval successful: {api_key[:10]}...")
            return True
        else:
            print("❌ API key retrieval failed - no key returned")
            return False

    except Exception as e:
        print(f"❌ API key retrieval error: {str(e)}")
        return False


async def main():
    """Run all Google AI Studio tests."""
    print("🚀 Starting Google AI Studio Integration Tests")
    print("=" * 50)

    # Test 1: API key retrieval
    key_success = await test_api_key_retrieval()

    # Test 2: Full integration test
    if key_success:
        integration_success = await test_google_ai_studio_integration()
    else:
        print("⏭️  Skipping integration test due to API key failure")
        integration_success = False

    # Summary
    print("\n" + "=" * 50)
    print("📊 TEST SUMMARY:")
    print(f"   API Key Retrieval: {'✅ PASS' if key_success else '❌ FAIL'}")
    print(f"   Full Integration: {'✅ PASS' if integration_success else '❌ FAIL'}")

    if key_success and integration_success:
        print(
            "\n🎉 All tests passed! Google AI Studio integration is working correctly."
        )
    else:
        print("\n⚠️  Some tests failed. Check your Google AI Studio configuration.")
        print("\nTroubleshooting tips:")
        print("1. Ensure GOOGLE_API_KEY is set in your .env file")
        print("2. Verify your API key is valid and has sufficient quota")
        print("3. Check that you have access to Gemini 2.0 Flash-Lite model")
        print("4. Consider further reducing concurrent operations if rate limited")


if __name__ == "__main__":
    asyncio.run(main())


src/test_openrouter_integration.py
"""Test file for OpenRouter integration with DeepSeek model."""  
  
import asyncio  
import uuid  
import os  
from dotenv import load_dotenv  
from open_deep_research.deep_researcher import deep_researcher_builder  
from langgraph.checkpoint.memory import MemorySaver  
  
load_dotenv()  
  
async def test_openrouter_integration():  
    """Test OpenRouter integration with DeepSeek model."""  
      
    # Configure OpenRouter with DeepSeek model  
    config = {  
        "configurable": {  
            "thread_id": str(uuid.uuid4()),  
            # OpenRouter configuration  
            "research_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
            "compression_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",   
            "final_report_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
            "summarization_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
              
            # Research parameters  
            "max_structured_output_retries": 3,  
            "allow_clarification": False,  
            "max_concurrent_research_units": 2,  # Keep low for testing  
            "search_api": "tavily",  
            "max_researcher_iterations": 2,  # Keep low for testing  
            "max_react_tool_calls": 5,  
              
            # Token limits  
            "research_model_max_tokens": 4000,  
            "compression_model_max_tokens": 4000,  
            "final_report_model_max_tokens": 4000,  
            "summarization_model_max_tokens": 2000,  
        }  
    }  
      
    # Simple test query  
    test_query = "What are the key benefits of renewable energy?"  
      
    try:  
        # Initialize the graph  
        graph = deep_researcher_builder.compile(checkpointer=MemorySaver())  
          
        print("🧪 Testing OpenRouter integration with DeepSeek...")  
        print(f"📝 Query: {test_query}")  
        print("⚙️  Model: deepseek/deepseek-chat-v3-0324:free")  
        print("-" * 50)  
          
        # Run the research  
        result = await graph.ainvoke(  
            {"messages": [{"role": "user", "content": test_query}]},  
            config  
        )  
          
        # Check results  
        if "final_report" in result and result["final_report"]:  
            print("✅ SUCCESS: OpenRouter integration working!")  
            print(f"📄 Report length: {len(result['final_report'])} characters")  
            print("\n📋 Generated Report Preview:")  
            print("-" * 30)  
            # Show first 500 characters of the report  
            preview = result["final_report"][:500]  
            print(f"{preview}...")  
            print("-" * 30)  
              
            return True  
        else:  
            print("❌ FAILED: No final report generated")  
            print(f"Result keys: {list(result.keys())}")  
            return False  
              
    except Exception as e:  
        print(f"❌ ERROR: {str(e)}")  
        print(f"Error type: {type(e).__name__}")  
        return False  
  
async def test_model_initialization():  
    """Test that the init_openrouter_model function works correctly."""  
    from open_deep_research.utils import init_openrouter_model  
      
    try:  
        print("\n🔧 Testing model initialization...")  
          
        # Get the API key for OpenRouter  
        api_key = os.getenv("OPENROUTER_API_KEY")  
        if not api_key:  
            print("❌ OPENROUTER_API_KEY not found in environment")  
            return False  
          
        print(f"🔑 Found API key: {api_key[:10]}...")  
          
        # Test OpenRouter model initialization with API key  
        model = init_openrouter_model(  
            "openrouter:openai/gpt-3.5-turbo",  
            max_tokens=1000,  
            api_key=api_key  # Pass the API key explicitly  
        )  
          
        # Simple test message  
        test_message = [{"role": "user", "content": "Hello, can you respond with 'OpenRouter test successful'?"}]  
          
        response = await model.ainvoke(test_message)  
          
        if response and hasattr(response, 'content'):  
            print("✅ Model initialization successful!")  
            print(f"📝 Response: {response.content[:100]}...")  
            return True  
        else:  
            print("❌ Model initialization failed - no response content")  
            return False  
              
    except Exception as e:  
        print(f"❌ Model initialization error: {str(e)}")  
        return False  
  
async def test_api_key_retrieval():  
    """Test that the get_api_key_for_model function works for OpenRouter."""  
    from open_deep_research.utils import get_api_key_for_model  
      
    try:  
        print("\n🔍 Testing API key retrieval...")  
          
        # Mock config for testing  
        mock_config = {"configurable": {}}  
          
        # Test OpenRouter key retrieval  
        api_key = get_api_key_for_model("openrouter:deepseek/deepseek-chat-v3-0324:free", mock_config)  
          
        if api_key:  
            print(f"✅ API key retrieval successful: {api_key[:10]}...")  
            return True  
        else:  
            print("❌ API key retrieval failed - no key returned")  
            return False  
              
    except Exception as e:  
        print(f"❌ API key retrieval error: {str(e)}")  
        return False  
  
async def main():  
    """Run all OpenRouter tests."""  
    print("🚀 Starting OpenRouter Integration Tests")  
    print("=" * 50)  
      
    # Test 1: API key retrieval  
    key_success = await test_api_key_retrieval()  
      
    # Test 2: Model initialization  
    init_success = await test_model_initialization()  
      
    # Test 3: Full integration test  
    if init_success:  
        integration_success = await test_openrouter_integration()  
    else:  
        print("⏭️  Skipping integration test due to initialization failure")  
        integration_success = False  
      
    # Summary  
    print("\n" + "=" * 50)  
    print("📊 TEST SUMMARY:")  
    print(f"   API Key Retrieval: {'✅ PASS' if key_success else '❌ FAIL'}")  
    print(f"   Model Initialization: {'✅ PASS' if init_success else '❌ FAIL'}")  
    print(f"   Full Integration: {'✅ PASS' if integration_success else '❌ FAIL'}")  
      
    if key_success and init_success and integration_success:  
        print("\n🎉 All tests passed! OpenRouter integration is working correctly.")  
    else:  
        print("\n⚠️  Some tests failed. Check your OpenRouter configuration.")  
        print("\nTroubleshooting tips:")  
        print("1. Ensure OPENROUTER_API_KEY is set in your .env file")  
        print("2. Verify your init_openrouter_model function is implemented in utils.py")  
        print("3. Check that get_api_key_for_model handles OpenRouter models")  
        print("4. Ensure your API key starts with 'sk-or-' and has sufficient credits")  
  
if __name__ == "__main__":  
    asyncio.run(main())


src/mcp_server.py
import os
import json
from typing import List
from fastmcp import FastMCP
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

app = FastMCP("Codebase Consulting Server")
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
model = genai.GenerativeModel("gemini-2.5-flash")

@app.tool()
def extract_relevant_file_content(query: str) -> str:
    """
    An MCP tool that extracts and concatenates content from relevant files
    that are useful for answering the user query.
    Returns a single string containing all relevant content.
    """
    prompt = f"""
    Extract concise keywords or relevant keywords (that may not be from the query but are relevant) from the following query that can be used to search relevant source files. DO NOT include any words that are not relevant to the query. Include only the MOST relevant keywords.
    Your response MUST be a list of the keywords seperated by spaces. Do not include any other text, comments, or markdown.

    Query: "{query}"
    """
    response = model.generate_content(prompt).text.strip()

    keywords = response.split()
    keywords = ["test_openrouter_integration"]

    print(f"DEBUG: Extracted keywords from query: {keywords}")

    relevant_information = ""
    for root, _, files in os.walk("."):
        for f in files:
            rel_path = os.path.relpath(os.path.join(root, f), ".")
            try:
                with open(rel_path, "r", encoding="utf-8", errors="ignore") as file:
                    content = file.read()
                # Check if any keyword is in filename or content
                if any(kw.lower() in rel_path.lower() or kw.lower() in content.lower() for kw in keywords):
                    relevant_information += rel_path + "\n" + content + "\n\n"
            except Exception as e:
                print(f"Error reading {rel_path}: {e}")

    return relevant_information.strip()

if __name__ == "__main__":
    app.run(transport="http", host="127.0.0.1", port=8080)
The `test_openrouter_integration.py` file contains the following query:

```
test_query = "What are the key benefits of renewable energy?"
```

================================================================================



test_outputs/google_ai_studio_full_report_20250829_103557.txt
Google AI Studio Integration Test - Full Report
Timestamp: 2025-08-29T10:35:57.681251
Query: What query is used in test_openrouter_integration file?
Model: gemini-2.0-flash-lite
================================================================================

I am unable to provide the specific query used in the `test_openrouter_integration` file because the `<Findings>` section is empty.
### References
[1] /path/to/test_openrouter_integration.py (Hypothetical, as no file path was provided)

.git/index
DIRC      /hh  :         mwr>A1x`2F .env.example      hyhy  =         T騁y1L ui (.github/workflows/claude-code-review.yml  hh  >         jslջ`{UG .github/workflows/claude.yml      hNhN  ?         h*#D/PÆ݁9o 
.gitignore        hRhR  @         ~п*\;֪K 	CLAUDE.md hh  A         )|C*uWm LICENSE   hShS  B         ([8UzX5ɨHtƍ` 	README.md h(h(  D         -1-gZppK$%<_?'O] examples/arxiv.md hh  E         .xcR;MH7 "examples/inference-market-gpt45.md        hh  F         'yoMg:?{G examples/inference-market.md      h^h^  G         5NKKRϑr?p examples/pubmed.md        hwhw  H         '8NOL9 langgraph.json    hbWhbW  I         $Ѵ826oe> pyproject.toml    hh  _         thI
0*p"r &src/code_consultant/code_consultant.py    h4Q"*h4Q"*  F         eEl<6
VH|; $src/code_consultant/configuration.py      h-Kh-K  눜         5<9Kѵ3US0wV|h% src/code_consultant/prompts.py    h3r~oh3r~o  a         oP-%]~Mk src/code_consultant/state.py      hbMhbM  E         c$'έʯ2

? src/code_consultant/utils.py      h,%h,%  L         Ŋ/O
l_P src/legacy/CLAUDE.md      h?h?  M          HU^g6" src/legacy/__init__.py    hLhL  N         땲_'\3Z src/legacy/configuration.py       h h   P         6h,"L src/legacy/files/vibe_code.md     hĉhĉ  Q         MIOEC6l src/legacy/graph.ipynb    hƼhƼ  R         R.>X]PQ9lCA src/legacy/graph.py       hh  S         P]\p3(NR"< src/legacy/legacy.md      hѯhѯ  T        v܊T+ĺ src/legacy/multi_agent.ipynb      hfhf  U         KtV7Be1X( src/legacy/multi_agent.py h|,h|,  V         P׀e89YdMՋ src/legacy/prompts.py     hh  W         j**ꮇmnqdLtb	 src/legacy/state.py       h6h6  Y         zLTLHl src/legacy/tests/conftest.py      hݑhݑ  Z         H4K$h
O>}݁
1 src/legacy/tests/run_test.py      hhhh  [         2t9vjnlϠtR
 'src/legacy/tests/test_report_quality.py   h4h4  \        5h8أ
\	< src/legacy/utils.py       hNh
v           uxJk.9ҔaLq src/mcp_server.py hh  d         UKb֘aIsߧ src/security/auth.py      h,#ìh+j  2         !T6fXbP (src/test_google_ai_studio_integration.py  h Zh G           yOȯgwiߒ\U "src/test_openrouter_integration.py        hh  f         %C59(-pO tests/evaluators.py       h+h+  h        5'Hhx*d ;tests/expt_results/deep_research_bench_claude4-sonnet.jsonl       hW
hW
  i        ^MeyQҳn7Ny$_ 4tests/expt_results/deep_research_bench_gpt-4.1.jsonl      h_h_  j         }(kV4~ei 2tests/expt_results/deep_research_bench_gpt-5.jsonl        h h   k         RgT
ZeT  tests/extract_langsmith_data.py   hc.hc.  l         [tPW#z"cfꩵ  tests/pairwise_evaluation.py      h3h3  m         $|WndZh8 tests/prompts.py  hh  n         ИT}5;8`kyMN tests/run_evaluate.py     hh  o         x|yX 'tests/supervisor_parallel_evaluation.py   hh  p        "qq$=DR1XV uv.lock   TREE   47 4
7(9W)7&src 24 3
򟊣|d3cU^H
legacy 15 2
PO;ef[Kfiles 1 0
K{yuNjlw(Etests 3 0
[D'Jdδd(security 1 0
@_cޢ@R+t"dcode_consultant 5 0
!3cmE	tests 9 1
\3;
!$H|7expt_results 3 0
Y  I1c.github 2 1
N+wͮ_}*[workflows 2 0
Ê>Mp}:kexamples 4 0
Qýs'o}P
F[z#7rw/;^Y#l#H&

src/test_google_ai_studio_integration.py
"""Test file for Google AI Studio integration with Gemini 2.0 Flash-Lite model."""

import asyncio
import json
import os
import uuid
from datetime import datetime

from dotenv import load_dotenv
from langgraph.checkpoint.memory import MemorySaver

from code_consultant.code_consultant import code_consultant
from code_consultant.utils import get_all_tools, get_api_key_for_model

load_dotenv()


async def test_google_ai_studio_integration():
    """Test Google AI Studio integration with Gemini 2.0 Flash-Lite model."""

    # Configure Google AI Studio with Gemini 2.0 Flash-Lite model
    config = {
        "configurable": {
            "thread_id": str(uuid.uuid4()),
            # Google AI Studio configuration
            "analyst_model": "google:gemini-2.0-flash-lite",
            "compression_model": "google:gemini-2.0-flash-lite",
            "final_report_model": "google:gemini-2.0-flash-lite",
            "summarization_model": "google:gemini-2.0-flash-lite",
            # Research parameters - reduced for rate limiting
            "max_structured_output_retries": 2,  # Reduced from 3
            "allow_clarification": False,
            "max_concurrent_tasks": 1,  # Reduced from 2 for rate limiting
            "max_iterations": 1,  # Reduced from 2 for rate limiting
            "max_react_tool_calls": 3,  # Reduced from 5 for rate limiting
            # Token limits - conservative for rate limiting
            "analyst_model_max_tokens": 1000000,  # Reduced from 4000
            "compression_model_max_tokens": 1000000,  # Reduced from 4000
            "final_report_model_max_tokens": 1000000,  # Reduced from 4000
            "summarization_model_max_tokens": 1000000,  # Reduced from 2000
            # mcp server config
            "mcp_config": {
                "url": "http://localhost:8080",  # HTTP URL instead of stdio
                "tools": ["extract_relevant_file_content"],
                "auth_required": False,
            },
            "mcp_prompt": """

**CRITICAL FILE READING INSTRUCTIONS:**
**FOR SUPERVISORS:** When asked about local files (like README, config files, etc.), you MUST delegate these tasks to analysts via ExecuteTask. The analysts have access to the `extract_relevant_file_content` tool. Do NOT refuse local file requests - delegate them!

**FOR ANALYSTS:** When conducting file-related analysis, you have access to:
1. `extract_relevant_file_content(query: str)`: To extract content from relevant files based on a query. The query should be a descriptive sentence about what you are looking for.
2. NEVER provide generic information about files when you can read the actual content.
3. Local file questions should ALWAYS use your file reading tools."""
        }
    }

    tools = await get_all_tools(config)  
    print(f"🔧 Available tools: {[tool.name if hasattr(tool, 'name') else str(tool) for tool in tools]}")  
    print(f"🔧 Total tools loaded: {len(tools)}")

    # Simple test query
    test_query = "What query is used in test_openrouter_integration file?"

    try:
        # Initialize the graph
        graph = code_consultant.compile(checkpointer=MemorySaver())

        print("🧪 Testing Google AI Studio integration with Gemini 2.0 Flash-Lite...")
        print(f"📝 Query: {test_query}")
        print("⚙️  Model: gemini-2.0-flash-lite")
        print("-" * 50)

        # Run the research
        result = await graph.ainvoke(
            {"messages": [{"role": "user", "content": test_query}]}, config
        )
        print(f"Supervisor messages: {result.get('supervisor_messages', [])}")  
        print(f"Task brief: {result.get('task_brief', '')}")

        # Save raw notes to file
        if "raw_notes" in result and result["raw_notes"]:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"google_ai_studio_raw_notes_{timestamp}.txt"

            print(f"📝 Raw notes found: {len(result['raw_notes'])} entries")

            # Create output directory if it doesn't exist
            os.makedirs("test_outputs", exist_ok=True)

            with open(f"test_outputs/{filename}", "w", encoding="utf-8") as f:
                f.write(f"Google AI Studio Integration Test - Raw Notes\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Query: {test_query}\n")
                f.write(f"Model: gemini-2.0-flash-lite\n")
                f.write("=" * 80 + "\n\n")

                for i, note in enumerate(result["raw_notes"]):
                    f.write(f"RAW NOTE {i+1}:\n")
                    f.write("-" * 40 + "\n")
                    f.write(str(note))
                    f.write("\n\n" + "=" * 80 + "\n\n")

            print(f"💾 Raw notes saved to: test_outputs/{filename}")

            # Also print a preview
            for i, note in enumerate(result["raw_notes"]):
                print(f"📄 Raw Note {i+1} Preview: {str(note)[:200]}...")
        else:
            print("📝 No raw notes found in result")
            print(f"Available result keys: {list(result.keys())}")

        # Check results
        if "final_report" in result and result["final_report"]:
            print("✅ SUCCESS: Google AI Studio integration working!")
            print(f"📄 Report length: {len(result['final_report'])} characters")

            # Also save the full report
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"google_ai_studio_full_report_{timestamp}.txt"

            with open(f"test_outputs/{report_filename}", "w", encoding="utf-8") as f:
                f.write(f"Google AI Studio Integration Test - Full Report\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Query: {test_query}\n")
                f.write(f"Model: gemini-2.0-flash-lite\n")
                f.write("=" * 80 + "\n\n")
                f.write(result["final_report"])

            print(f"💾 Full report saved to: test_outputs/{report_filename}")

            print("\n📋 Generated Report Preview:")
            print("-" * 30)
            # Show first 500 characters of the report
            preview = result["final_report"][:500]
            print(f"{preview}...")
            print("-" * 30)

            return True
        else:
            print("❌ FAILED: No final report generated")
            print(f"Result keys: {list(result.keys())}")
            return False

    except Exception as e:
        print(f"❌ ERROR: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        return False


async def test_api_key_retrieval():
    """Test that the get_api_key_for_model function works for Google models."""

    try:
        print("\n🔍 Testing API key retrieval...")

        # Mock config for testing
        mock_config = {"configurable": {}}

        # Test Google key retrieval
        api_key = get_api_key_for_model("google:gemini-2.0-flash-lite", mock_config)

        if api_key:
            print(f"✅ API key retrieval successful: {api_key[:10]}...")
            return True
        else:
            print("❌ API key retrieval failed - no key returned")
            return False

    except Exception as e:
        print(f"❌ API key retrieval error: {str(e)}")
        return False


async def main():
    """Run all Google AI Studio tests."""
    print("🚀 Starting Google AI Studio Integration Tests")
    print("=" * 50)

    # Test 1: API key retrieval
    key_success = await test_api_key_retrieval()

    # Test 2: Full integration test
    if key_success:
        integration_success = await test_google_ai_studio_integration()
    else:
        print("⏭️  Skipping integration test due to API key failure")
        integration_success = False

    # Summary
    print("\n" + "=" * 50)
    print("📊 TEST SUMMARY:")
    print(f"   API Key Retrieval: {'✅ PASS' if key_success else '❌ FAIL'}")
    print(f"   Full Integration: {'✅ PASS' if integration_success else '❌ FAIL'}")

    if key_success and integration_success:
        print(
            "\n🎉 All tests passed! Google AI Studio integration is working correctly."
        )
    else:
        print("\n⚠️  Some tests failed. Check your Google AI Studio configuration.")
        print("\nTroubleshooting tips:")
        print("1. Ensure GOOGLE_API_KEY is set in your .env file")
        print("2. Verify your API key is valid and has sufficient quota")
        print("3. Check that you have access to Gemini 2.0 Flash-Lite model")
        print("4. Consider further reducing concurrent operations if rate limited")


if __name__ == "__main__":
    asyncio.run(main())


src/test_openrouter_integration.py
"""Test file for OpenRouter integration with DeepSeek model."""  
  
import asyncio  
import uuid  
import os  
from dotenv import load_dotenv  
from open_deep_research.deep_researcher import deep_researcher_builder  
from langgraph.checkpoint.memory import MemorySaver  
  
load_dotenv()  
  
async def test_openrouter_integration():  
    """Test OpenRouter integration with DeepSeek model."""  
      
    # Configure OpenRouter with DeepSeek model  
    config = {  
        "configurable": {  
            "thread_id": str(uuid.uuid4()),  
            # OpenRouter configuration  
            "research_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
            "compression_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",   
            "final_report_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
            "summarization_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
              
            # Research parameters  
            "max_structured_output_retries": 3,  
            "allow_clarification": False,  
            "max_concurrent_research_units": 2,  # Keep low for testing  
            "search_api": "tavily",  
            "max_researcher_iterations": 2,  # Keep low for testing  
            "max_react_tool_calls": 5,  
              
            # Token limits  
            "research_model_max_tokens": 4000,  
            "compression_model_max_tokens": 4000,  
            "final_report_model_max_tokens": 4000,  
            "summarization_model_max_tokens": 2000,  
        }  
    }  
      
    # Simple test query  
    test_query = "What are the key benefits of renewable energy?"  
      
    try:  
        # Initialize the graph  
        graph = deep_researcher_builder.compile(checkpointer=MemorySaver())  
          
        print("🧪 Testing OpenRouter integration with DeepSeek...")  
        print(f"📝 Query: {test_query}")  
        print("⚙️  Model: deepseek/deepseek-chat-v3-0324:free")  
        print("-" * 50)  
          
        # Run the research  
        result = await graph.ainvoke(  
            {"messages": [{"role": "user", "content": test_query}]},  
            config  
        )  
          
        # Check results  
        if "final_report" in result and result["final_report"]:  
            print("✅ SUCCESS: OpenRouter integration working!")  
            print(f"📄 Report length: {len(result['final_report'])} characters")  
            print("\n📋 Generated Report Preview:")  
            print("-" * 30)  
            # Show first 500 characters of the report  
            preview = result["final_report"][:500]  
            print(f"{preview}...")  
            print("-" * 30)  
              
            return True  
        else:  
            print("❌ FAILED: No final report generated")  
            print(f"Result keys: {list(result.keys())}")  
            return False  
              
    except Exception as e:  
        print(f"❌ ERROR: {str(e)}")  
        print(f"Error type: {type(e).__name__}")  
        return False  
  
async def test_model_initialization():  
    """Test that the init_openrouter_model function works correctly."""  
    from open_deep_research.utils import init_openrouter_model  
      
    try:  
        print("\n🔧 Testing model initialization...")  
          
        # Get the API key for OpenRouter  
        api_key = os.getenv("OPENROUTER_API_KEY")  
        if not api_key:  
            print("❌ OPENROUTER_API_KEY not found in environment")  
            return False  
          
        print(f"🔑 Found API key: {api_key[:10]}...")  
          
        # Test OpenRouter model initialization with API key  
        model = init_openrouter_model(  
            "openrouter:openai/gpt-3.5-turbo",  
            max_tokens=1000,  
            api_key=api_key  # Pass the API key explicitly  
        )  
          
        # Simple test message  
        test_message = [{"role": "user", "content": "Hello, can you respond with 'OpenRouter test successful'?"}]  
          
        response = await model.ainvoke(test_message)  
          
        if response and hasattr(response, 'content'):  
            print("✅ Model initialization successful!")  
            print(f"📝 Response: {response.content[:100]}...")  
            return True  
        else:  
            print("❌ Model initialization failed - no response content")  
            return False  
              
    except Exception as e:  
        print(f"❌ Model initialization error: {str(e)}")  
        return False  
  
async def test_api_key_retrieval():  
    """Test that the get_api_key_for_model function works for OpenRouter."""  
    from open_deep_research.utils import get_api_key_for_model  
      
    try:  
        print("\n🔍 Testing API key retrieval...")  
          
        # Mock config for testing  
        mock_config = {"configurable": {}}  
          
        # Test OpenRouter key retrieval  
        api_key = get_api_key_for_model("openrouter:deepseek/deepseek-chat-v3-0324:free", mock_config)  
          
        if api_key:  
            print(f"✅ API key retrieval successful: {api_key[:10]}...")  
            return True  
        else:  
            print("❌ API key retrieval failed - no key returned")  
            return False  
              
    except Exception as e:  
        print(f"❌ API key retrieval error: {str(e)}")  
        return False  
  
async def main():  
    """Run all OpenRouter tests."""  
    print("🚀 Starting OpenRouter Integration Tests")  
    print("=" * 50)  
      
    # Test 1: API key retrieval  
    key_success = await test_api_key_retrieval()  
      
    # Test 2: Model initialization  
    init_success = await test_model_initialization()  
      
    # Test 3: Full integration test  
    if init_success:  
        integration_success = await test_openrouter_integration()  
    else:  
        print("⏭️  Skipping integration test due to initialization failure")  
        integration_success = False  
      
    # Summary  
    print("\n" + "=" * 50)  
    print("📊 TEST SUMMARY:")  
    print(f"   API Key Retrieval: {'✅ PASS' if key_success else '❌ FAIL'}")  
    print(f"   Model Initialization: {'✅ PASS' if init_success else '❌ FAIL'}")  
    print(f"   Full Integration: {'✅ PASS' if integration_success else '❌ FAIL'}")  
      
    if key_success and init_success and integration_success:  
        print("\n🎉 All tests passed! OpenRouter integration is working correctly.")  
    else:  
        print("\n⚠️  Some tests failed. Check your OpenRouter configuration.")  
        print("\nTroubleshooting tips:")  
        print("1. Ensure OPENROUTER_API_KEY is set in your .env file")  
        print("2. Verify your init_openrouter_model function is implemented in utils.py")  
        print("3. Check that get_api_key_for_model handles OpenRouter models")  
        print("4. Ensure your API key starts with 'sk-or-' and has sufficient credits")  
  
if __name__ == "__main__":  
    asyncio.run(main())


src/mcp_server.py
import os
import json
from typing import List
from fastmcp import FastMCP
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

app = FastMCP("Codebase Consulting Server")
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
model = genai.GenerativeModel("gemini-2.5-flash")

@app.tool()
def extract_relevant_file_content(query: str) -> str:
    """
    An MCP tool that extracts and concatenates content from relevant files
    that are useful for answering the user query.
    Returns a single string containing all relevant content.
    """
    prompt = f"""
    Extract concise keywords or relevant keywords (that may not be from the query but are relevant) from the following query that can be used to search relevant source files. DO NOT include any words that are not relevant to the query. Include only the MOST relevant keywords.
    Your response MUST be a list of the keywords seperated by spaces. Do not include any other text, comments, or markdown.

    Query: "{query}"
    """
    response = model.generate_content(prompt).text.strip()

    keywords = response.split()
    keywords = ["test_openrouter_integration"]

    print(f"DEBUG: Extracted keywords from query: {keywords}")

    relevant_information = ""
    for root, _, files in os.walk("."):
        for f in files:
            rel_path = os.path.relpath(os.path.join(root, f), ".")
            try:
                with open(rel_path, "r", encoding="utf-8", errors="ignore") as file:
                    content = file.read()
                # Check if any keyword is in filename or content
                if any(kw.lower() in rel_path.lower() or kw.lower() in content.lower() for kw in keywords):
                    relevant_information += rel_path + "\n" + content + "\n\n"
            except Exception as e:
                print(f"Error reading {rel_path}: {e}")

    return relevant_information.strip()

if __name__ == "__main__":
    app.run(transport="http", host="127.0.0.1", port=8080)
The query used in the `test_openrouter_integration.py` file is: `"What are the key benefits of renewable energy?"`

================================================================================



test_outputs/google_ai_studio_full_report_20250830_104542.txt
Google AI Studio Integration Test - Full Report
Timestamp: 2025-08-30T10:45:42.085286
Query: What query is used in test_openrouter_integration file?
Model: gemini-2.0-flash-lite
================================================================================

The `test_openrouter_integration.py` file uses the following query:

*   `What are the key benefits of renewable energy?`

### References
[1] /Users/euclides/Documents/Programming/Projects/open_deep_research/src/test_openrouter_integration.py

test_outputs/google_ai_studio_raw_notes_20250830_104542.txt
Google AI Studio Integration Test - Raw Notes
Timestamp: 2025-08-30T10:45:42.084421
Query: What query is used in test_openrouter_integration file?
Model: gemini-2.0-flash-lite
================================================================================

RAW NOTE 1:
----------------------------------------
Okay, I will start by using the `extract_relevant_file_content` tool to get the content of the 'test_openrouter_integration' file.
.mypy_cache/3.13/test_openrouter_integration.data.json
{".class":"MypyFile","_fullname":"test_openrouter_integration","future_import_flags":[],"is_partial_stub_package":false,"is_stub":false,"names":{".class":"SymbolTable","MemorySaver":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.MemorySaver","name":"MemorySaver","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.MemorySaver","source_any":null,"type_of_any":3}}},"__annotations__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__annotations__","name":"__annotations__","setter_type":null,"type":{".class":"Instance","args":["builtins.str",{".class":"AnyType","missing_import_name":null,"source_any":null,"type_of_any":6}],"extra_attrs":null,"type_ref":"builtins.dict"}}},"__doc__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__doc__","name":"__doc__","setter_type":null,"type":"builtins.str"}},"__file__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__file__","name":"__file__","setter_type":null,"type":"builtins.str"}},"__name__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__name__","name":"__name__","setter_type":null,"type":"builtins.str"}},"__package__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__package__","name":"__package__","setter_type":null,"type":"builtins.str"}},"__spec__":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_ready"],"fullname":"test_openrouter_integration.__spec__","name":"__spec__","setter_type":null,"type":"_frozen_importlib.ModuleSpec"}},"asyncio":{".class":"SymbolTableNode","cross_ref":"asyncio","kind":"Gdef"},"deep_researcher_builder":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.deep_researcher_builder","name":"deep_researcher_builder","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.deep_researcher_builder","source_any":null,"type_of_any":3}}},"load_dotenv":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"Var","flags":["is_suppressed_import","is_ready","is_inferred"],"fullname":"test_openrouter_integration.load_dotenv","name":"load_dotenv","setter_type":null,"type":{".class":"AnyType","missing_import_name":"test_openrouter_integration.load_dotenv","source_any":null,"type_of_any":3}}},"main":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.main","name":"main","type":null}},"os":{".class":"SymbolTableNode","cross_ref":"os","kind":"Gdef"},"test_api_key_retrieval":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_api_key_retrieval","name":"test_api_key_retrieval","type":null}},"test_model_initialization":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_model_initialization","name":"test_model_initialization","type":null}},"test_openrouter_integration":{".class":"SymbolTableNode","kind":"Gdef","node":{".class":"FuncDef","abstract_status":0,"arg_kinds":[],"arg_names":[],"dataclass_transform_spec":null,"deprecated":null,"flags":["is_coroutine"],"fullname":"test_openrouter_integration.test_openrouter_integration","name":"test_openrouter_integration","type":null}},"uuid":{".class":"SymbolTableNode","cross_ref":"uuid","kind":"Gdef"}},"path":"/Users/euclides/Documents/Programming/Projects/open_deep_research/src/test_openrouter_integration.py"}

.mypy_cache/3.13/test_openrouter_integration.meta.json
{"data_mtime":1756511441,"dep_lines":[3,4,5,1,1,1,1,1,1,1,8,6,7,83],"dep_prios":[10,10,10,5,30,30,30,30,30,30,5,5,5,20],"dependencies":["asyncio","uuid","os","builtins","_collections_abc","_frozen_importlib","_typeshed","abc","asyncio.events","typing"],"hash":"2306685f8c92a1b79021ff6256b5247c2611879b","id":"test_openrouter_integration","ignore_all":false,"interface_hash":"6b411363c6cfd3982a3ef831cfedc81ee96ef6b2","mtime":1756511441,"options":{"allow_redefinition":true,"allow_redefinition_new":false,"allow_untyped_globals":true,"always_false":[],"always_true":[],"bazel":false,"check_untyped_defs":false,"disable_bytearray_promotion":false,"disable_error_code":["import-untyped","import-not-found","misc","unused-ignore","name-defined","attr-defined","no-untyped-def","no-untyped-call","valid-type"],"disable_memoryview_promotion":false,"disabled_error_codes":["attr-defined","import-not-found","import-untyped","misc","name-defined","no-untyped-call","no-untyped-def","unused-ignore","valid-type"],"disallow_any_decorated":false,"disallow_any_explicit":false,"disallow_any_expr":false,"disallow_any_generics":false,"disallow_any_unimported":false,"disallow_incomplete_defs":false,"disallow_subclassing_any":false,"disallow_untyped_calls":false,"disallow_untyped_decorators":false,"disallow_untyped_defs":false,"enable_error_code":[],"enabled_error_codes":[],"extra_checks":false,"follow_imports":"silent","follow_imports_for_stubs":false,"follow_untyped_imports":false,"ignore_errors":false,"ignore_missing_imports":true,"implicit_optional":false,"implicit_reexport":true,"local_partial_types":false,"mypyc":false,"old_type_inference":false,"platform":"darwin","plugins":[],"strict_bytes":false,"strict_concatenate":false,"strict_equality":false,"strict_optional":false,"warn_no_return":false,"warn_return_any":false,"warn_unreachable":false,"warn_unused_ignores":false},"path":"/Users/euclides/Documents/Programming/Projects/open_deep_research/src/test_openrouter_integration.py","plugin_data":null,"size":7119,"suppressed":["langgraph.checkpoint.memory","dotenv","open_deep_research.deep_researcher","open_deep_research.utils"],"version_id":"1.16.1"}

test_outputs/google_ai_studio_full_report_20250829_103557.txt
Google AI Studio Integration Test - Full Report
Timestamp: 2025-08-29T10:35:57.681251
Query: What query is used in test_openrouter_integration file?
Model: gemini-2.0-flash-lite
================================================================================

I am unable to provide the specific query used in the `test_openrouter_integration` file because the `<Findings>` section is empty.
### References
[1] /path/to/test_openrouter_integration.py (Hypothetical, as no file path was provided)

.git/index
DIRC      /hh  :         mwr>A1x`2F .env.example      hyhy  =         T騁y1L ui (.github/workflows/claude-code-review.yml  hh  >         jslջ`{UG .github/workflows/claude.yml      hNhN  ?         h*#D/PÆ݁9o 
.gitignore        hRhR  @         ~п*\;֪K 	CLAUDE.md hh  A         )|C*uWm LICENSE   hShS  B         ([8UzX5ɨHtƍ` 	README.md h(h(  D         -1-gZppK$%<_?'O] examples/arxiv.md hh  E         .xcR;MH7 "examples/inference-market-gpt45.md        hh  F         'yoMg:?{G examples/inference-market.md      h^h^  G         5NKKRϑr?p examples/pubmed.md        hwhw  H         '8NOL9 langgraph.json    hbWhbW  I         $Ѵ826oe> pyproject.toml    hh  _         thI
0*p"r &src/code_consultant/code_consultant.py    h4Q"*h4Q"*  F         eEl<6
VH|; $src/code_consultant/configuration.py      h-Kh-K  눜         5<9Kѵ3US0wV|h% src/code_consultant/prompts.py    h3r~oh3r~o  a         oP-%]~Mk src/code_consultant/state.py      hbMhbM  E         c$'έʯ2

? src/code_consultant/utils.py      h,%h,%  L         Ŋ/O
l_P src/legacy/CLAUDE.md      h?h?  M          HU^g6" src/legacy/__init__.py    hLhL  N         땲_'\3Z src/legacy/configuration.py       h h   P         6h,"L src/legacy/files/vibe_code.md     hĉhĉ  Q         MIOEC6l src/legacy/graph.ipynb    hƼhƼ  R         R.>X]PQ9lCA src/legacy/graph.py       hh  S         P]\p3(NR"< src/legacy/legacy.md      hѯhѯ  T        v܊T+ĺ src/legacy/multi_agent.ipynb      hfhf  U         KtV7Be1X( src/legacy/multi_agent.py h|,h|,  V         P׀e89YdMՋ src/legacy/prompts.py     hh  W         j**ꮇmnqdLtb	 src/legacy/state.py       h6h6  Y         zLTLHl src/legacy/tests/conftest.py      hݑhݑ  Z         H4K$h
O>}݁
1 src/legacy/tests/run_test.py      hhhh  [         2t9vjnlϠtR
 'src/legacy/tests/test_report_quality.py   h4h4  \        5h8أ
\	< src/legacy/utils.py       hNh
v           uxJk.9ҔaLq src/mcp_server.py hh  d         UKb֘aIsߧ src/security/auth.py      h,#ìh+j  2         !T6fXbP (src/test_google_ai_studio_integration.py  h Zh G           yOȯgwiߒ\U "src/test_openrouter_integration.py        hh  f         %C59(-pO tests/evaluators.py       h+h+  h        5'Hhx*d ;tests/expt_results/deep_research_bench_claude4-sonnet.jsonl       hW
hW
  i        ^MeyQҳn7Ny$_ 4tests/expt_results/deep_research_bench_gpt-4.1.jsonl      h_h_  j         }(kV4~ei 2tests/expt_results/deep_research_bench_gpt-5.jsonl        h h   k         RgT
ZeT  tests/extract_langsmith_data.py   hc.hc.  l         [tPW#z"cfꩵ  tests/pairwise_evaluation.py      h3h3  m         $|WndZh8 tests/prompts.py  hh  n         ИT}5;8`kyMN tests/run_evaluate.py     hh  o         x|yX 'tests/supervisor_parallel_evaluation.py   hh  p        "qq$=DR1XV uv.lock   TREE   47 4
7(9W)7&src 24 3
򟊣|d3cU^H
legacy 15 2
PO;ef[Kfiles 1 0
K{yuNjlw(Etests 3 0
[D'Jdδd(security 1 0
@_cޢ@R+t"dcode_consultant 5 0
!3cmE	tests 9 1
\3;
!$H|7expt_results 3 0
Y  I1c.github 2 1
N+wͮ_}*[workflows 2 0
Ê>Mp}:kexamples 4 0
Qýs'o}P
F[z#7rw/;^Y#l#H&

src/test_google_ai_studio_integration.py
"""Test file for Google AI Studio integration with Gemini 2.0 Flash-Lite model."""

import asyncio
import json
import os
import uuid
from datetime import datetime

from dotenv import load_dotenv
from langgraph.checkpoint.memory import MemorySaver

from code_consultant.code_consultant import code_consultant
from code_consultant.utils import get_all_tools, get_api_key_for_model

load_dotenv()


async def test_google_ai_studio_integration():
    """Test Google AI Studio integration with Gemini 2.0 Flash-Lite model."""

    # Configure Google AI Studio with Gemini 2.0 Flash-Lite model
    config = {
        "configurable": {
            "thread_id": str(uuid.uuid4()),
            # Google AI Studio configuration
            "analyst_model": "google:gemini-2.0-flash-lite",
            "compression_model": "google:gemini-2.0-flash-lite",
            "final_report_model": "google:gemini-2.0-flash-lite",
            "summarization_model": "google:gemini-2.0-flash-lite",
            # Research parameters - reduced for rate limiting
            "max_structured_output_retries": 2,  # Reduced from 3
            "allow_clarification": False,
            "max_concurrent_tasks": 1,  # Reduced from 2 for rate limiting
            "max_iterations": 1,  # Reduced from 2 for rate limiting
            "max_react_tool_calls": 3,  # Reduced from 5 for rate limiting
            # Token limits - conservative for rate limiting
            "analyst_model_max_tokens": 1000000,  # Reduced from 4000
            "compression_model_max_tokens": 1000000,  # Reduced from 4000
            "final_report_model_max_tokens": 1000000,  # Reduced from 4000
            "summarization_model_max_tokens": 1000000,  # Reduced from 2000
            # mcp server config
            "mcp_config": {
                "url": "http://localhost:8080",  # HTTP URL instead of stdio
                "tools": ["extract_relevant_file_content"],
                "auth_required": False,
            },
            "mcp_prompt": """

**CRITICAL FILE READING INSTRUCTIONS:**
**FOR SUPERVISORS:** When asked about local files (like README, config files, etc.), you MUST delegate these tasks to analysts via ExecuteTask. The analysts have access to the `extract_relevant_file_content` tool. Do NOT refuse local file requests - delegate them!

**FOR ANALYSTS:** When conducting file-related analysis, you have access to:
1. `extract_relevant_file_content(query: str)`: To extract content from relevant files based on a query. The query should be a descriptive sentence about what you are looking for.
2. NEVER provide generic information about files when you can read the actual content.
3. Local file questions should ALWAYS use your file reading tools."""
        }
    }

    tools = await get_all_tools(config)  
    print(f"🔧 Available tools: {[tool.name if hasattr(tool, 'name') else str(tool) for tool in tools]}")  
    print(f"🔧 Total tools loaded: {len(tools)}")

    # Simple test query
    test_query = "What query is used in test_openrouter_integration file?"

    try:
        # Initialize the graph
        graph = code_consultant.compile(checkpointer=MemorySaver())

        print("🧪 Testing Google AI Studio integration with Gemini 2.0 Flash-Lite...")
        print(f"📝 Query: {test_query}")
        print("⚙️  Model: gemini-2.0-flash-lite")
        print("-" * 50)

        # Run the research
        result = await graph.ainvoke(
            {"messages": [{"role": "user", "content": test_query}]}, config
        )
        print(f"Supervisor messages: {result.get('supervisor_messages', [])}")  
        print(f"Task brief: {result.get('task_brief', '')}")

        # Save raw notes to file
        if "raw_notes" in result and result["raw_notes"]:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"google_ai_studio_raw_notes_{timestamp}.txt"

            print(f"📝 Raw notes found: {len(result['raw_notes'])} entries")

            # Create output directory if it doesn't exist
            os.makedirs("test_outputs", exist_ok=True)

            with open(f"test_outputs/{filename}", "w", encoding="utf-8") as f:
                f.write(f"Google AI Studio Integration Test - Raw Notes\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Query: {test_query}\n")
                f.write(f"Model: gemini-2.0-flash-lite\n")
                f.write("=" * 80 + "\n\n")

                for i, note in enumerate(result["raw_notes"]):
                    f.write(f"RAW NOTE {i+1}:\n")
                    f.write("-" * 40 + "\n")
                    f.write(str(note))
                    f.write("\n\n" + "=" * 80 + "\n\n")

            print(f"💾 Raw notes saved to: test_outputs/{filename}")

            # Also print a preview
            for i, note in enumerate(result["raw_notes"]):
                print(f"📄 Raw Note {i+1} Preview: {str(note)[:200]}...")
        else:
            print("📝 No raw notes found in result")
            print(f"Available result keys: {list(result.keys())}")

        # Check results
        if "final_report" in result and result["final_report"]:
            print("✅ SUCCESS: Google AI Studio integration working!")
            print(f"📄 Report length: {len(result['final_report'])} characters")

            # Also save the full report
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"google_ai_studio_full_report_{timestamp}.txt"

            with open(f"test_outputs/{report_filename}", "w", encoding="utf-8") as f:
                f.write(f"Google AI Studio Integration Test - Full Report\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Query: {test_query}\n")
                f.write(f"Model: gemini-2.0-flash-lite\n")
                f.write("=" * 80 + "\n\n")
                f.write(result["final_report"])

            print(f"💾 Full report saved to: test_outputs/{report_filename}")

            print("\n📋 Generated Report Preview:")
            print("-" * 30)
            # Show first 500 characters of the report
            preview = result["final_report"][:500]
            print(f"{preview}...")
            print("-" * 30)

            return True
        else:
            print("❌ FAILED: No final report generated")
            print(f"Result keys: {list(result.keys())}")
            return False

    except Exception as e:
        print(f"❌ ERROR: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        return False


async def test_api_key_retrieval():
    """Test that the get_api_key_for_model function works for Google models."""

    try:
        print("\n🔍 Testing API key retrieval...")

        # Mock config for testing
        mock_config = {"configurable": {}}

        # Test Google key retrieval
        api_key = get_api_key_for_model("google:gemini-2.0-flash-lite", mock_config)

        if api_key:
            print(f"✅ API key retrieval successful: {api_key[:10]}...")
            return True
        else:
            print("❌ API key retrieval failed - no key returned")
            return False

    except Exception as e:
        print(f"❌ API key retrieval error: {str(e)}")
        return False


async def main():
    """Run all Google AI Studio tests."""
    print("🚀 Starting Google AI Studio Integration Tests")
    print("=" * 50)

    # Test 1: API key retrieval
    key_success = await test_api_key_retrieval()

    # Test 2: Full integration test
    if key_success:
        integration_success = await test_google_ai_studio_integration()
    else:
        print("⏭️  Skipping integration test due to API key failure")
        integration_success = False

    # Summary
    print("\n" + "=" * 50)
    print("📊 TEST SUMMARY:")
    print(f"   API Key Retrieval: {'✅ PASS' if key_success else '❌ FAIL'}")
    print(f"   Full Integration: {'✅ PASS' if integration_success else '❌ FAIL'}")

    if key_success and integration_success:
        print(
            "\n🎉 All tests passed! Google AI Studio integration is working correctly."
        )
    else:
        print("\n⚠️  Some tests failed. Check your Google AI Studio configuration.")
        print("\nTroubleshooting tips:")
        print("1. Ensure GOOGLE_API_KEY is set in your .env file")
        print("2. Verify your API key is valid and has sufficient quota")
        print("3. Check that you have access to Gemini 2.0 Flash-Lite model")
        print("4. Consider further reducing concurrent operations if rate limited")


if __name__ == "__main__":
    asyncio.run(main())


src/test_openrouter_integration.py
"""Test file for OpenRouter integration with DeepSeek model."""  
  
import asyncio  
import uuid  
import os  
from dotenv import load_dotenv  
from open_deep_research.deep_researcher import deep_researcher_builder  
from langgraph.checkpoint.memory import MemorySaver  
  
load_dotenv()  
  
async def test_openrouter_integration():  
    """Test OpenRouter integration with DeepSeek model."""  
      
    # Configure OpenRouter with DeepSeek model  
    config = {  
        "configurable": {  
            "thread_id": str(uuid.uuid4()),  
            # OpenRouter configuration  
            "research_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
            "compression_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",   
            "final_report_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
            "summarization_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
              
            # Research parameters  
            "max_structured_output_retries": 3,  
            "allow_clarification": False,  
            "max_concurrent_research_units": 2,  # Keep low for testing  
            "search_api": "tavily",  
            "max_researcher_iterations": 2,  # Keep low for testing  
            "max_react_tool_calls": 5,  
              
            # Token limits  
            "research_model_max_tokens": 4000,  
            "compression_model_max_tokens": 4000,  
            "final_report_model_max_tokens": 4000,  
            "summarization_model_max_tokens": 2000,  
        }  
    }  
      
    # Simple test query  
    test_query = "What are the key benefits of renewable energy?"  
      
    try:  
        # Initialize the graph  
        graph = deep_researcher_builder.compile(checkpointer=MemorySaver())  
          
        print("🧪 Testing OpenRouter integration with DeepSeek...")  
        print(f"📝 Query: {test_query}")  
        print("⚙️  Model: deepseek/deepseek-chat-v3-0324:free")  
        print("-" * 50)  
          
        # Run the research  
        result = await graph.ainvoke(  
            {"messages": [{"role": "user", "content": test_query}]},  
            config  
        )  
          
        # Check results  
        if "final_report" in result and result["final_report"]:  
            print("✅ SUCCESS: OpenRouter integration working!")  
            print(f"📄 Report length: {len(result['final_report'])} characters")  
            print("\n📋 Generated Report Preview:")  
            print("-" * 30)  
            # Show first 500 characters of the report  
            preview = result["final_report"][:500]  
            print(f"{preview}...")  
            print("-" * 30)  
              
            return True  
        else:  
            print("❌ FAILED: No final report generated")  
            print(f"Result keys: {list(result.keys())}")  
            return False  
              
    except Exception as e:  
        print(f"❌ ERROR: {str(e)}")  
        print(f"Error type: {type(e).__name__}")  
        return False  
  
async def test_model_initialization():  
    """Test that the init_openrouter_model function works correctly."""  
    from open_deep_research.utils import init_openrouter_model  
      
    try:  
        print("\n🔧 Testing model initialization...")  
          
        # Get the API key for OpenRouter  
        api_key = os.getenv("OPENROUTER_API_KEY")  
        if not api_key:  
            print("❌ OPENROUTER_API_KEY not found in environment")  
            return False  
          
        print(f"🔑 Found API key: {api_key[:10]}...")  
          
        # Test OpenRouter model initialization with API key  
        model = init_openrouter_model(  
            "openrouter:openai/gpt-3.5-turbo",  
            max_tokens=1000,  
            api_key=api_key  # Pass the API key explicitly  
        )  
          
        # Simple test message  
        test_message = [{"role": "user", "content": "Hello, can you respond with 'OpenRouter test successful'?"}]  
          
        response = await model.ainvoke(test_message)  
          
        if response and hasattr(response, 'content'):  
            print("✅ Model initialization successful!")  
            print(f"📝 Response: {response.content[:100]}...")  
            return True  
        else:  
            print("❌ Model initialization failed - no response content")  
            return False  
              
    except Exception as e:  
        print(f"❌ Model initialization error: {str(e)}")  
        return False  
  
async def test_api_key_retrieval():  
    """Test that the get_api_key_for_model function works for OpenRouter."""  
    from open_deep_research.utils import get_api_key_for_model  
      
    try:  
        print("\n🔍 Testing API key retrieval...")  
          
        # Mock config for testing  
        mock_config = {"configurable": {}}  
          
        # Test OpenRouter key retrieval  
        api_key = get_api_key_for_model("openrouter:deepseek/deepseek-chat-v3-0324:free", mock_config)  
          
        if api_key:  
            print(f"✅ API key retrieval successful: {api_key[:10]}...")  
            return True  
        else:  
            print("❌ API key retrieval failed - no key returned")  
            return False  
              
    except Exception as e:  
        print(f"❌ API key retrieval error: {str(e)}")  
        return False  
  
async def main():  
    """Run all OpenRouter tests."""  
    print("🚀 Starting OpenRouter Integration Tests")  
    print("=" * 50)  
      
    # Test 1: API key retrieval  
    key_success = await test_api_key_retrieval()  
      
    # Test 2: Model initialization  
    init_success = await test_model_initialization()  
      
    # Test 3: Full integration test  
    if init_success:  
        integration_success = await test_openrouter_integration()  
    else:  
        print("⏭️  Skipping integration test due to initialization failure")  
        integration_success = False  
      
    # Summary  
    print("\n" + "=" * 50)  
    print("📊 TEST SUMMARY:")  
    print(f"   API Key Retrieval: {'✅ PASS' if key_success else '❌ FAIL'}")  
    print(f"   Model Initialization: {'✅ PASS' if init_success else '❌ FAIL'}")  
    print(f"   Full Integration: {'✅ PASS' if integration_success else '❌ FAIL'}")  
      
    if key_success and init_success and integration_success:  
        print("\n🎉 All tests passed! OpenRouter integration is working correctly.")  
    else:  
        print("\n⚠️  Some tests failed. Check your OpenRouter configuration.")  
        print("\nTroubleshooting tips:")  
        print("1. Ensure OPENROUTER_API_KEY is set in your .env file")  
        print("2. Verify your init_openrouter_model function is implemented in utils.py")  
        print("3. Check that get_api_key_for_model handles OpenRouter models")  
        print("4. Ensure your API key starts with 'sk-or-' and has sufficient credits")  
  
if __name__ == "__main__":  
    asyncio.run(main())


src/mcp_server.py
import os
import json
from typing import List
from fastmcp import FastMCP
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

app = FastMCP("Codebase Consulting Server")
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
model = genai.GenerativeModel("gemini-2.5-flash")

@app.tool()
def extract_relevant_file_content(query: str) -> str:
    """
    An MCP tool that extracts and concatenates content from relevant files
    that are useful for answering the user query.
    Returns a single string containing all relevant content.
    """
    prompt = f"""
    Extract concise keywords or relevant keywords (that may not be from the query but are relevant) from the following query that can be used to search relevant source files. DO NOT include any words that are not relevant to the query. Include only the MOST relevant keywords.
    Your response MUST be a list of the keywords seperated by spaces. Do not include any other text, comments, or markdown.

    Query: "{query}"
    """
    response = model.generate_content(prompt).text.strip()

    keywords = response.split()
    keywords = ["test_openrouter_integration"]

    print(f"DEBUG: Extracted keywords from query: {keywords}")

    relevant_information = ""
    for root, _, files in os.walk("."):
        for f in files:
            rel_path = os.path.relpath(os.path.join(root, f), ".")
            try:
                with open(rel_path, "r", encoding="utf-8", errors="ignore") as file:
                    content = file.read()
                # Check if any keyword is in filename or content
                if any(kw.lower() in rel_path.lower() or kw.lower() in content.lower() for kw in keywords):
                    relevant_information += rel_path + "\n" + content + "\n\n"
            except Exception as e:
                print(f"Error reading {rel_path}: {e}")

    return relevant_information.strip()

if __name__ == "__main__":
    app.run(transport="http", host="127.0.0.1", port=8080)
The `test_openrouter_integration.py` file contains the following query:

```
test_query = "What are the key benefits of renewable energy?"
```

================================================================================



test_outputs/google_ai_studio_full_report_20250829_103557.txt
Google AI Studio Integration Test - Full Report
Timestamp: 2025-08-29T10:35:57.681251
Query: What query is used in test_openrouter_integration file?
Model: gemini-2.0-flash-lite
================================================================================

I am unable to provide the specific query used in the `test_openrouter_integration` file because the `<Findings>` section is empty.
### References
[1] /path/to/test_openrouter_integration.py (Hypothetical, as no file path was provided)

.git/index
DIRC      /hh  :         mwr>A1x`2F .env.example      hyhy  =         T騁y1L ui (.github/workflows/claude-code-review.yml  hh  >         jslջ`{UG .github/workflows/claude.yml      hNhN  ?         h*#D/PÆ݁9o 
.gitignore        hRhR  @         ~п*\;֪K 	CLAUDE.md hh  A         )|C*uWm LICENSE   hShS  B         ([8UzX5ɨHtƍ` 	README.md h(h(  D         -1-gZppK$%<_?'O] examples/arxiv.md hh  E         .xcR;MH7 "examples/inference-market-gpt45.md        hh  F         'yoMg:?{G examples/inference-market.md      h^h^  G         5NKKRϑr?p examples/pubmed.md        hwhw  H         '8NOL9 langgraph.json    hbWhbW  I         $Ѵ826oe> pyproject.toml    hh  _         thI
0*p"r &src/code_consultant/code_consultant.py    h4Q"*h4Q"*  F         eEl<6
VH|; $src/code_consultant/configuration.py      h-Kh-K  눜         5<9Kѵ3US0wV|h% src/code_consultant/prompts.py    h3r~oh3r~o  a         oP-%]~Mk src/code_consultant/state.py      hbMhbM  E         c$'έʯ2

? src/code_consultant/utils.py      h,%h,%  L         Ŋ/O
l_P src/legacy/CLAUDE.md      h?h?  M          HU^g6" src/legacy/__init__.py    hLhL  N         땲_'\3Z src/legacy/configuration.py       h h   P         6h,"L src/legacy/files/vibe_code.md     hĉhĉ  Q         MIOEC6l src/legacy/graph.ipynb    hƼhƼ  R         R.>X]PQ9lCA src/legacy/graph.py       hh  S         P]\p3(NR"< src/legacy/legacy.md      hѯhѯ  T        v܊T+ĺ src/legacy/multi_agent.ipynb      hfhf  U         KtV7Be1X( src/legacy/multi_agent.py h|,h|,  V         P׀e89YdMՋ src/legacy/prompts.py     hh  W         j**ꮇmnqdLtb	 src/legacy/state.py       h6h6  Y         zLTLHl src/legacy/tests/conftest.py      hݑhݑ  Z         H4K$h
O>}݁
1 src/legacy/tests/run_test.py      hhhh  [         2t9vjnlϠtR
 'src/legacy/tests/test_report_quality.py   h4h4  \        5h8أ
\	< src/legacy/utils.py       hNh
v           uxJk.9ҔaLq src/mcp_server.py hh  d         UKb֘aIsߧ src/security/auth.py      h,#ìh+j  2         !T6fXbP (src/test_google_ai_studio_integration.py  h Zh G           yOȯgwiߒ\U "src/test_openrouter_integration.py        hh  f         %C59(-pO tests/evaluators.py       h+h+  h        5'Hhx*d ;tests/expt_results/deep_research_bench_claude4-sonnet.jsonl       hW
hW
  i        ^MeyQҳn7Ny$_ 4tests/expt_results/deep_research_bench_gpt-4.1.jsonl      h_h_  j         }(kV4~ei 2tests/expt_results/deep_research_bench_gpt-5.jsonl        h h   k         RgT
ZeT  tests/extract_langsmith_data.py   hc.hc.  l         [tPW#z"cfꩵ  tests/pairwise_evaluation.py      h3h3  m         $|WndZh8 tests/prompts.py  hh  n         ИT}5;8`kyMN tests/run_evaluate.py     hh  o         x|yX 'tests/supervisor_parallel_evaluation.py   hh  p        "qq$=DR1XV uv.lock   TREE   47 4
7(9W)7&src 24 3
򟊣|d3cU^H
legacy 15 2
PO;ef[Kfiles 1 0
K{yuNjlw(Etests 3 0
[D'Jdδd(security 1 0
@_cޢ@R+t"dcode_consultant 5 0
!3cmE	tests 9 1
\3;
!$H|7expt_results 3 0
Y  I1c.github 2 1
N+wͮ_}*[workflows 2 0
Ê>Mp}:kexamples 4 0
Qýs'o}P
F[z#7rw/;^Y#l#H&

src/test_google_ai_studio_integration.py
"""Test file for Google AI Studio integration with Gemini 2.0 Flash-Lite model."""

import asyncio
import json
import os
import uuid
from datetime import datetime

from dotenv import load_dotenv
from langgraph.checkpoint.memory import MemorySaver

from code_consultant.code_consultant import code_consultant
from code_consultant.utils import get_all_tools, get_api_key_for_model

load_dotenv()


async def test_google_ai_studio_integration():
    """Test Google AI Studio integration with Gemini 2.0 Flash-Lite model."""

    # Configure Google AI Studio with Gemini 2.0 Flash-Lite model
    config = {
        "configurable": {
            "thread_id": str(uuid.uuid4()),
            # Google AI Studio configuration
            "analyst_model": "google:gemini-2.0-flash-lite",
            "compression_model": "google:gemini-2.0-flash-lite",
            "final_report_model": "google:gemini-2.0-flash-lite",
            "summarization_model": "google:gemini-2.0-flash-lite",
            "max_structured_output_retries": 2,
            "allow_clarification": False,
            "max_concurrent_tasks": 1,
            "max_iterations": 1,
            "max_react_tool_calls": 3,
            "analyst_model_max_tokens": 1000000,
            "compression_model_max_tokens": 1000000,
            "final_report_model_max_tokens": 1000000,
            "summarization_model_max_tokens": 1000000,
            # mcp server config
            "mcp_config": {
                "url": "http://localhost:8080", 
                "tools": ["extract_relevant_file_content"],
                "auth_required": False,
            },
            "mcp_prompt": """

**CRITICAL FILE READING INSTRUCTIONS:**
**FOR SUPERVISORS:** When asked about local files (like README, config files, etc.), you MUST delegate these tasks to analysts via ExecuteTask. The analysts have access to the `extract_relevant_file_content` tool. Do NOT refuse local file requests - delegate them!

**FOR ANALYSTS:** When conducting file-related analysis, you have access to:
1. `extract_relevant_file_content(query: str)`: To extract content from relevant files based on a query. The query should be a descriptive sentence about what you are looking for.
2. NEVER provide generic information about files when you can read the actual content.
3. Local file questions should ALWAYS use your file reading tools."""
        }
    }

    tools = await get_all_tools(config)  
    print(f"🔧 Available tools: {[tool.name if hasattr(tool, 'name') else str(tool) for tool in tools]}")  
    print(f"🔧 Total tools loaded: {len(tools)}")

    # Simple test query
    test_query = "What model is used in test_openrouter_integration file?"

    try:
        # Initialize the graph
        graph = code_consultant.compile(checkpointer=MemorySaver())

        print("🧪 Testing Google AI Studio integration with Gemini 2.0 Flash-Lite...")
        print(f"📝 Query: {test_query}")
        print("⚙️  Model: gemini-2.0-flash-lite")
        print("-" * 50)

        # Run the research
        result = await graph.ainvoke(
            {"messages": [{"role": "user", "content": test_query}]}, config
        )
        print(f"Supervisor messages: {result.get('supervisor_messages', [])}")  
        print(f"Task brief: {result.get('task_brief', '')}")

        # Save raw notes to file
        if "raw_notes" in result and result["raw_notes"]:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"google_ai_studio_raw_notes_{timestamp}.txt"

            print(f"📝 Raw notes found: {len(result['raw_notes'])} entries")

            # Create output directory if it doesn't exist
            os.makedirs("test_outputs", exist_ok=True)

            with open(f"test_outputs/{filename}", "w", encoding="utf-8") as f:
                f.write(f"Google AI Studio Integration Test - Raw Notes\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Query: {test_query}\n")
                f.write(f"Model: gemini-2.0-flash-lite\n")
                f.write("=" * 80 + "\n\n")

                for i, note in enumerate(result["raw_notes"]):
                    f.write(f"RAW NOTE {i+1}:\n")
                    f.write("-" * 40 + "\n")
                    f.write(str(note))
                    f.write("\n\n" + "=" * 80 + "\n\n")

            print(f"💾 Raw notes saved to: test_outputs/{filename}")

            # Also print a preview
            for i, note in enumerate(result["raw_notes"]):
                print(f"📄 Raw Note {i+1} Preview: {str(note)[:200]}...")
        else:
            print("📝 No raw notes found in result")
            print(f"Available result keys: {list(result.keys())}")

        # Check results
        if "final_report" in result and result["final_report"]:
            print("✅ SUCCESS: Google AI Studio integration working!")
            print(f"📄 Report length: {len(result['final_report'])} characters")

            # Also save the full report
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"google_ai_studio_full_report_{timestamp}.txt"

            with open(f"test_outputs/{report_filename}", "w", encoding="utf-8") as f:
                f.write(f"Google AI Studio Integration Test - Full Report\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Query: {test_query}\n")
                f.write(f"Model: gemini-2.0-flash-lite\n")
                f.write("=" * 80 + "\n\n")
                f.write(result["final_report"])

            print(f"💾 Full report saved to: test_outputs/{report_filename}")

            print("\n📋 Generated Report Preview:")
            print("-" * 30)
            # Show first 500 characters of the report
            preview = result["final_report"][:500]
            print(f"{preview}...")
            print("-" * 30)

            return True
        else:
            print("❌ FAILED: No final report generated")
            print(f"Result keys: {list(result.keys())}")
            return False

    except Exception as e:
        print(f"❌ ERROR: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        return False


async def test_api_key_retrieval():
    """Test that the get_api_key_for_model function works for Google models."""

    try:
        print("\n🔍 Testing API key retrieval...")

        # Mock config for testing
        mock_config = {"configurable": {}}

        # Test Google key retrieval
        api_key = get_api_key_for_model("google:gemini-2.0-flash-lite", mock_config)

        if api_key:
            print(f"✅ API key retrieval successful: {api_key[:10]}...")
            return True
        else:
            print("❌ API key retrieval failed - no key returned")
            return False

    except Exception as e:
        print(f"❌ API key retrieval error: {str(e)}")
        return False


async def main():
    """Run all Google AI Studio tests."""
    print("🚀 Starting Google AI Studio Integration Tests")
    print("=" * 50)

    # Test 1: API key retrieval
    key_success = await test_api_key_retrieval()

    # Test 2: Full integration test
    if key_success:
        integration_success = await test_google_ai_studio_integration()
    else:
        print("⏭️  Skipping integration test due to API key failure")
        integration_success = False

    # Summary
    print("\n" + "=" * 50)
    print("📊 TEST SUMMARY:")
    print(f"   API Key Retrieval: {'✅ PASS' if key_success else '❌ FAIL'}")
    print(f"   Full Integration: {'✅ PASS' if integration_success else '❌ FAIL'}")

    if key_success and integration_success:
        print(
            "\n🎉 All tests passed! Google AI Studio integration is working correctly."
        )
    else:
        print("\n⚠️  Some tests failed. Check your Google AI Studio configuration.")
        print("\nTroubleshooting tips:")
        print("1. Ensure GOOGLE_API_KEY is set in your .env file")
        print("2. Verify your API key is valid and has sufficient quota")
        print("3. Check that you have access to Gemini 2.0 Flash-Lite model")
        print("4. Consider further reducing concurrent operations if rate limited")


if __name__ == "__main__":
    asyncio.run(main())


src/test_openrouter_integration.py
"""Test file for OpenRouter integration with DeepSeek model."""  
  
import asyncio  
import uuid  
import os  
from dotenv import load_dotenv  
from open_deep_research.deep_researcher import deep_researcher_builder  
from langgraph.checkpoint.memory import MemorySaver  
  
load_dotenv()  
  
async def test_openrouter_integration():  
    """Test OpenRouter integration with DeepSeek model."""  
      
    # Configure OpenRouter with DeepSeek model  
    config = {  
        "configurable": {  
            "thread_id": str(uuid.uuid4()),  
            # OpenRouter configuration  
            "research_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
            "compression_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",   
            "final_report_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
            "summarization_model": "openrouter:deepseek/deepseek-chat-v3-0324:free",  
              
            # Research parameters  
            "max_structured_output_retries": 3,  
            "allow_clarification": False,  
            "max_concurrent_research_units": 2,  # Keep low for testing  
            "search_api": "tavily",  
            "max_researcher_iterations": 2,  # Keep low for testing  
            "max_react_tool_calls": 5,  
              
            # Token limits  
            "research_model_max_tokens": 4000,  
            "compression_model_max_tokens": 4000,  
            "final_report_model_max_tokens": 4000,  
            "summarization_model_max_tokens": 2000,  
        }  
    }  
      
    # Simple test query  
    test_query = "What are the key benefits of renewable energy?"  
      
    try:  
        # Initialize the graph  
        graph = deep_researcher_builder.compile(checkpointer=MemorySaver())  
          
        print("🧪 Testing OpenRouter integration with DeepSeek...")  
        print(f"📝 Query: {test_query}")  
        print("⚙️  Model: deepseek/deepseek-chat-v3-0324:free")  
        print("-" * 50)  
          
        # Run the research  
        result = await graph.ainvoke(  
            {"messages": [{"role": "user", "content": test_query}]},  
            config  
        )  
          
        # Check results  
        if "final_report" in result and result["final_report"]:  
            print("✅ SUCCESS: OpenRouter integration working!")  
            print(f"📄 Report length: {len(result['final_report'])} characters")  
            print("\n📋 Generated Report Preview:")  
            print("-" * 30)  
            # Show first 500 characters of the report  
            preview = result["final_report"][:500]  
            print(f"{preview}...")  
            print("-" * 30)  
              
            return True  
        else:  
            print("❌ FAILED: No final report generated")  
            print(f"Result keys: {list(result.keys())}")  
            return False  
              
    except Exception as e:  
        print(f"❌ ERROR: {str(e)}")  
        print(f"Error type: {type(e).__name__}")  
        return False  
  
async def test_model_initialization():  
    """Test that the init_openrouter_model function works correctly."""  
    from open_deep_research.utils import init_openrouter_model  
      
    try:  
        print("\n🔧 Testing model initialization...")  
          
        # Get the API key for OpenRouter  
        api_key = os.getenv("OPENROUTER_API_KEY")  
        if not api_key:  
            print("❌ OPENROUTER_API_KEY not found in environment")  
            return False  
          
        print(f"🔑 Found API key: {api_key[:10]}...")  
          
        # Test OpenRouter model initialization with API key  
        model = init_openrouter_model(  
            "openrouter:openai/gpt-3.5-turbo",  
            max_tokens=1000,  
            api_key=api_key  # Pass the API key explicitly  
        )  
          
        # Simple test message  
        test_message = [{"role": "user", "content": "Hello, can you respond with 'OpenRouter test successful'?"}]  
          
        response = await model.ainvoke(test_message)  
          
        if response and hasattr(response, 'content'):  
            print("✅ Model initialization successful!")  
            print(f"📝 Response: {response.content[:100]}...")  
            return True  
        else:  
            print("❌ Model initialization failed - no response content")  
            return False  
              
    except Exception as e:  
        print(f"❌ Model initialization error: {str(e)}")  
        return False  
  
async def test_api_key_retrieval():  
    """Test that the get_api_key_for_model function works for OpenRouter."""  
    from open_deep_research.utils import get_api_key_for_model  
      
    try:  
        print("\n🔍 Testing API key retrieval...")  
          
        # Mock config for testing  
        mock_config = {"configurable": {}}  
          
        # Test OpenRouter key retrieval  
        api_key = get_api_key_for_model("openrouter:deepseek/deepseek-chat-v3-0324:free", mock_config)  
          
        if api_key:  
            print(f"✅ API key retrieval successful: {api_key[:10]}...")  
            return True  
        else:  
            print("❌ API key retrieval failed - no key returned")  
            return False  
              
    except Exception as e:  
        print(f"❌ API key retrieval error: {str(e)}")  
        return False  
  
async def main():  
    """Run all OpenRouter tests."""  
    print("🚀 Starting OpenRouter Integration Tests")  
    print("=" * 50)  
      
    # Test 1: API key retrieval  
    key_success = await test_api_key_retrieval()  
      
    # Test 2: Model initialization  
    init_success = await test_model_initialization()  
      
    # Test 3: Full integration test  
    if init_success:  
        integration_success = await test_openrouter_integration()  
    else:  
        print("⏭️  Skipping integration test due to initialization failure")  
        integration_success = False  
      
    # Summary  
    print("\n" + "=" * 50)  
    print("📊 TEST SUMMARY:")  
    print(f"   API Key Retrieval: {'✅ PASS' if key_success else '❌ FAIL'}")  
    print(f"   Model Initialization: {'✅ PASS' if init_success else '❌ FAIL'}")  
    print(f"   Full Integration: {'✅ PASS' if integration_success else '❌ FAIL'}")  
      
    if key_success and init_success and integration_success:  
        print("\n🎉 All tests passed! OpenRouter integration is working correctly.")  
    else:  
        print("\n⚠️  Some tests failed. Check your OpenRouter configuration.")  
        print("\nTroubleshooting tips:")  
        print("1. Ensure OPENROUTER_API_KEY is set in your .env file")  
        print("2. Verify your init_openrouter_model function is implemented in utils.py")  
        print("3. Check that get_api_key_for_model handles OpenRouter models")  
        print("4. Ensure your API key starts with 'sk-or-' and has sufficient credits")  
  
if __name__ == "__main__":  
    asyncio.run(main())


src/mcp_server.py
import os
import json
from typing import List
from fastmcp import FastMCP
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

app = FastMCP("Codebase Consulting Server")
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
model = genai.GenerativeModel("gemini-2.5-flash")

@app.tool()
def extract_relevant_file_content(query: str) -> str:
    """
    An MCP tool that extracts and concatenates content from relevant files
    that are useful for answering the user query.
    Returns a single string containing all relevant content.
    """
    prompt = f"""
    Extract concise keywords or relevant keywords (that may not be from the query but are relevant) from the following query that can be used to search relevant source files. DO NOT include any words that are not relevant to the query. Include only the MOST relevant keywords.
    Your response MUST be a list of the keywords seperated by spaces. Do not include any other text, comments, or markdown.

    Query: "{query}"
    """
    response = model.generate_content(prompt).text.strip()

    keywords = response.split()
    keywords = ["test_openrouter_integration"]

    print(f"DEBUG: Extracted keywords from query: {keywords}")

    relevant_information = ""
    for root, _, files in os.walk("."):
        for f in files:
            rel_path = os.path.relpath(os.path.join(root, f), ".")
            try:
                with open(rel_path, "r", encoding="utf-8", errors="ignore") as file:
                    content = file.read()
                # Check if any keyword is in filename or content
                if any(kw.lower() in rel_path.lower() or kw.lower() in content.lower() for kw in keywords):
                    relevant_information += rel_path + "\n" + content + "\n\n"
            except Exception as e:
                print(f"Error reading {rel_path}: {e}")

    return relevant_information.strip()

if __name__ == "__main__":
    app.run(transport="http", host="127.0.0.1", port=8080)
The `test_openrouter_integration.py` file uses the `openrouter:deepseek/deepseek-chat-v3-0324:free` model.

================================================================================

